// app/api/analise/route.js
import { NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import OpenAI from "openai";

// =========================
// üî• Fun√ß√£o GEMINI (principal) ‚Äì sem NENHUMA altera√ß√£o
// =========================
async function gerarComGemini(prompt) {
  const apiKey = process.env.GEMINI_API_KEY;
  const ai = new GoogleGenerativeAI(apiKey);

  const model = ai.getGenerativeModel({
    model: "gemini-2.5-flash",
    tools: [{ googleSearch: {} }], // Pesquisa REAL ‚ö°
  });

  const maxTentativas = 3;

  for (let i = 0; i < maxTentativas; i++) {
    try {
      console.log(`üîé Gemini tentativa ${i + 1}/${maxTentativas}`);
      const response = await model.generateContent({
        contents: [{ role: "user", parts: [{ text: prompt }] }],
      });

      const text = response.response.text();
      if (!text) throw new Error("Resposta vazia do Gemini");

      return { ok: true, text };
    } catch (error) {
      if (error.status === 503 || error.message.includes("overloaded")) {
        console.log("‚ö†Ô∏è Gemini sobrecarregado ‚Äî retry‚Ä¶");
        await new Promise(res => setTimeout(res, 1200));
        continue;
      }

      console.log("‚ùå Erro no Gemini:", error.message);
      return { ok: false, error };
    }
  }

  return { ok: false, error: new Error("Gemini falhou ap√≥s retries") };
}

// =========================
// üî• PROMPT EXTRA PARA FALLBACK (texto grande estilo Gemini)
// =========================
function montarPromptFallback(promptOriginal) {
  return `
INSTRU√á√ïES IMPORTANTES (MODO FALLBACK):

Voc√™ N√ÉO tem acesso √† internet.  
Portanto, gere uma an√°lise EXTREMAMENTE COMPLETA e DETALHADA, 
seguindo rigorosamente o padr√£o da Betgram IA.

üìå O texto DEVE ter NO M√çNIMO **600 palavras**.
üìå Analise SEMPRE TODOS os mercados listados abaixo:

- Resultado Final (1X2)
- Over/Under 2.5 gols
- Ambas Marcam (BTTS)
- Escanteios (Over/Under)
- Cart√µes (Over/Under)
- Valor Esperado (EV)
- Odds justas
- Conclus√µes detalhadas para CADA mercado

Siga o estilo visual da Betgram IA:
- T√≠tulos com emojis
- Destaques com cores (n√£o coloque tags HTML)
- Explica√ß√µes passo a passo
- Probabilidades estimadas
- Recomenda√ß√µes claras

‚ö† Ignore completamente a falta de dados reais.  
Use estat√≠sticas T√çPICAS dos times, padr√µes ofensivos/defensivos e 
conhecimento geral do futebol para estimar m√©dias.

AGORA RESPONDA COM BASE NO PROMPT ORIGINAL:

"${promptOriginal}"
`;
}

// =========================
// üî• Fallback 1 ‚Äî GPT-5-mini
// =========================
async function gerarComGPT5(promptOriginal) {
  try {
    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    console.log("üü† Fallback ‚Üí GPT-5-mini‚Ä¶");

    const prompt = montarPromptFallback(promptOriginal);

    const completion = await client.chat.completions.create({
      model: "gpt-5-mini",
      messages: [{ role: "user", content: prompt }],
      max_tokens: 5000,
      temperature: 0.7,
    });

    return { ok: true, text: completion.choices[0].message.content };
  } catch (error) {
    console.log("‚ùå Erro no GPT-5-mini:", error.message);
    return { ok: false, error };
  }
}

// =========================
// üî• Fallback 2 ‚Äî GPT-4o-mini
// =========================
async function gerarComGPT4(promptOriginal) {
  try {
    const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    console.log("üü° Fallback ‚Üí GPT-4o-mini‚Ä¶");

    const prompt = montarPromptFallback(promptOriginal);

    const completion = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: prompt }],
      max_tokens: 5000,
      temperature: 0.7,
    });

    return { ok: true, text: completion.choices[0].message.content };
  } catch (error) {
    console.log("‚ùå Erro no GPT-4o-mini:", error.message);
    return { ok: false, error };
  }
}

// =========================
// üî• ROTA PRINCIPAL
// =========================
export async function POST(req) {
  try {
    const { prompt } = await req.json();

    if (!prompt || prompt.trim().length < 3) {
      return NextResponse.json({ error: "Prompt inv√°lido." }, { status: 400 });
    }

    // 1Ô∏è‚É£ GEMINI (principal)
    const gemini = await gerarComGemini(prompt);
    if (gemini.ok) {
      return NextResponse.json({ content: gemini.text, fallback: false });
    }

    console.log("‚ö†Ô∏è Gemini falhou ‚Äî fallback para GPT-5-mini.");

    // 2Ô∏è‚É£ FALLBACK GPT-5
    const gpt5 = await gerarComGPT5(prompt);
    if (gpt5.ok) {
      return NextResponse.json({ content: gpt5.text, fallback: true });
    }

    console.log("‚ö†Ô∏è GPT-5-mini falhou ‚Äî fallback para GPT-4o-mini.");

    // 3Ô∏è‚É£ FALLBACK GPT-4
    const gpt4 = await gerarComGPT4(prompt);
    if (gpt4.ok) {
      return NextResponse.json({ content: gpt4.text, fallback: true });
    }

    // 4Ô∏è‚É£ Nada funcionou
    return NextResponse.json(
      { error: "Nenhum modelo conseguiu gerar resposta." },
      { status: 500 }
    );
  } catch (error) {
    console.error("üî• ERRO GERAL:", error);
    return NextResponse.json(
      { error: error.message || "Erro desconhecido" },
      { status: 500 }
    );
  }
}

